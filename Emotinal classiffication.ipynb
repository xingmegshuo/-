{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import jieba\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sudo pip3 install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有用户评论数据分为正负样本，通过lstm训练来得到一个模型，通过该型我们预测一句评论是正样本或者是负样本\n",
    "1.将所有评论汇总\n",
    "2.通过结巴分词和word2vec生成词嵌量\n",
    "3.使用keras进行训练返回模型\n",
    "4.测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_list = Textsta('/home/zer0/桌面/用户评论情感分析/中文情感分析语料库/clothing/neg.txt')\n",
    "# 这个只需运行一次\n",
    "# 读取所有的语料文本，写入一个总文件同时生成标签\n",
    "# 以neg.txt 结尾负样本,以pos.txt结尾正样本\n",
    "y=[]\n",
    "def create_all(filename,y):\n",
    "    with open('all.txt','a+') as t:\n",
    "        f = open('中文情感分析语料库'+filename,'r',encoding='gbk',errors='ignore')\n",
    "        data = f.read()\n",
    "        t.write(data+'\\n')\n",
    "        count = open('中文情感分析语料库'+filename,'r',encoding='gbk',errors='ignore').readlines()\n",
    "        for i in range(len(count)):\n",
    "#             print(filename[-7:])\n",
    "            if filename[-7:]=='neg.txt':\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "        f.close()\n",
    "        t.close()\n",
    "\n",
    "file_list = ['/clothing/neg.txt','/clothing/pos.txt','/fruit/neg.txt','/fruit/pos.txt','/hotel/neg.txt','/hotel/pos.txt','/pda/neg.txt','/pda/pos.txt','/shampoo/neg.txt','/shampoo/pos.txt']\n",
    "\n",
    "for i in file_list:\n",
    "    create_all(i,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 传进一个文件名，返回该文件分词后的列表\n",
    "\n",
    "def Textsta(filename): \n",
    "    f = open(filename,'r',encoding='utf-8')\n",
    "    sentences_list = []\n",
    "    for line in f:\n",
    "        single_list = line.strip().split(' ')\n",
    "        single_list = jieba.analyse.extract_tags(single_list[0],topK=20,withWeight=False,allowPOS=())\n",
    "        while '' in single_list:\n",
    "            single_list.remove('')\n",
    "        sentences_list.append(single_list)\n",
    "    f.close()\n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用word2vec 模型返回字典和词的向量\n",
    "\n",
    "def create_dictionaries(model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(),allow_update=True)\n",
    "    w2indx = {v:k+1 for k,v in gensim_dict.items()}\n",
    "    w2vec = {word:model[word] for word in w2indx.keys()}\n",
    "    return w2indx,w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.756 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05399641 -0.10912966  0.12273714 -0.04977618  0.08095848  0.0123782\n",
      " -0.04039728 -0.07805733 -0.04894287  0.11321828  0.06093043  0.1166272\n",
      "  0.00669917 -0.00227427 -0.01535578 -0.01468829  0.04204039 -0.07025021\n",
      "  0.16036604  0.00890837 -0.03197939 -0.10754133  0.06049434  0.08962453\n",
      " -0.0033539   0.08726767  0.0532543  -0.01957593  0.04428588  0.0558311\n",
      "  0.10793772  0.0305222   0.04336311 -0.01237232  0.03882742 -0.09393188\n",
      "  0.0361244  -0.08009711  0.09666263  0.05550296 -0.01417902  0.04177978\n",
      "  0.13866548  0.02448069  0.0995876   0.07853965  0.05106522 -0.03618902\n",
      " -0.05889981  0.16454238  0.14964625 -0.10103108  0.00411489 -0.13777688\n",
      " -0.06397779  0.0111174  -0.02855334 -0.10617605 -0.01923109  0.03118331\n",
      " -0.02526609  0.01191539 -0.01318542 -0.01041531  0.01357579 -0.02841787\n",
      " -0.06332406  0.06217002 -0.06881575  0.05133618  0.03116947  0.01810011\n",
      "  0.00404764  0.12125523 -0.04075477 -0.05919284 -0.13271192  0.03628181\n",
      "  0.10039835  0.06864693  0.03264365 -0.01654016 -0.01427459  0.00250531\n",
      " -0.06967897 -0.05437307  0.00300382  0.00738962 -0.11710127  0.18471144\n",
      "  0.07760006 -0.05539511  0.03817118  0.06555004 -0.02068945  0.00662707\n",
      " -0.04941734 -0.1349197   0.06993718 -0.00895508]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#对all.txt 分词\n",
    "sentence = Textsta('all.txt')\n",
    "# word2vec 方法转向量\n",
    "model = Word2Vec(sentence,size=100,min_count=5,window=5)\n",
    "index_dict,word_vectors = create_dictionaries(model)\n",
    "# print(index_dict)\n",
    "print(word_vectors['熟练'])\n",
    "# 可以看到这个词在字典中的表示就是这样的，我们也可以保存到本地\n",
    "# with open('word_vec.txt','w') as f:\n",
    "#     f.write(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7767, 100)\n"
     ]
    }
   ],
   "source": [
    "# 生成词嵌入向量\n",
    "# 把上边一个词向量的长度转为100\n",
    "n_symbols = len(index_dict) +1\n",
    "embedding_weights = np.zeros((n_symbols,100))\n",
    "for w,index in index_dict.items():\n",
    "    embedding_weights[index,:] = word_vectors[w]\n",
    "print(embedding_weights.shape)\n",
    "# 词有7767个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在字典中找到词返回索引\n",
    "# 文本和词典匹配将我们的词特征转为数字\n",
    "def text_to_index_array(dic,sentence):\n",
    "    new_sentence = []\n",
    "    for sen in sentence:\n",
    "        new_sen = []\n",
    "        for word in sen:\n",
    "            try:\n",
    "                new_sen.append(dic[word])\n",
    "            except:\n",
    "                new_sen.append(0)\n",
    "        new_sentence.append(new_sen)\n",
    "    return np.array(new_sentence)\n",
    "\n",
    "x = text_to_index_array(index_dict,sentence)\n",
    "# print(x[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense,Dropout,Activation\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 50)\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集生成训练和测试4:1\n",
    "# sklearn包划分数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
    "# 把x输入特征标准化，不够的补0是每一个输入的x 为50长度\n",
    "x_train = sequence.pad_sequences(x_train,maxlen=50)\n",
    "x_test = sequence.pad_sequences(x_test,maxlen=50)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(recurrent_activation=\"hard_sigmoid\", units=50, activation=\"sigmoid\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=100,input_dim=n_symbols,mask_zero=True,weights=[embedding_weights]))\n",
    "model.add(LSTM(output_dim=50,activation='sigmoid',inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 44s 1ms/step - loss: 0.4036 - acc: 0.8091 - val_loss: 0.2648 - val_acc: 0.8954\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 44s 1ms/step - loss: 0.2461 - acc: 0.9041 - val_loss: 0.2591 - val_acc: 0.8999\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 0.2140 - acc: 0.9186 - val_loss: 0.2556 - val_acc: 0.9016\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.1954 - acc: 0.9271 - val_loss: 0.2656 - val_acc: 0.9013\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.1818 - acc: 0.9328 - val_loss: 0.2767 - val_acc: 0.8981\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.1707 - acc: 0.9385 - val_loss: 0.2958 - val_acc: 0.8944\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.1625 - acc: 0.9413 - val_loss: 0.3093 - val_acc: 0.8929\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 47s 1ms/step - loss: 0.1562 - acc: 0.9448 - val_loss: 0.3205 - val_acc: 0.8894\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 44s 1ms/step - loss: 0.1499 - acc: 0.9460 - val_loss: 0.3253 - val_acc: 0.8906\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 46s 1ms/step - loss: 0.1439 - acc: 0.9479 - val_loss: 0.3499 - val_acc: 0.8901\n",
      "10000/10000 [==============================] - 4s 379us/step\n",
      "0.3499092327058315 0.8901\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         776700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 806,951\n",
      "Trainable params: 806,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=64,epochs=10,validation_data=(x_test,y_test))\n",
    "score,acc = model.evaluate(x_test,y_test,batch_size=32)\n",
    "print(score,acc)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# 直接输入一句话对其进行预测\n",
    "def convert_vector_predict(str_r):\n",
    "    new_str = jieba.analyse.extract_tags(str_r,topK=20,withWeight=False,allowPOS=())\n",
    "#     print(new_str)\n",
    "    x = text_to_index_array(index_dict,[new_str])\n",
    "    x = sequence.pad_sequences(x,maxlen=50)\n",
    "#     print(x)\n",
    "    y = model.predict_classes(x)\n",
    "    return y\n",
    "print(convert_vector_predict('好可爱啊'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
