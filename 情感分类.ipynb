{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用LSTM + word2vec词向量进行文本情感分类/情感分析实验\n",
    "利用Keras + LSTM进行文本分类\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "功能：利用词向量+LSTM进行文本分类\n",
    "时间：2017年3月10日 21:18:34\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from Functions import GetLineList\n",
    "from Functions.TextSta import TextSta\n",
    "\n",
    "# 参数设置\n",
    "vocab_dim = 100  # 向量维度\n",
    "maxlen = 140  # 文本保留的最大长度\n",
    "batch_size = 32\n",
    "n_epoch = 5\n",
    "input_length = 140\n",
    "\n",
    "\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in sen:\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "\n",
    "\n",
    "# 定义网络结构\n",
    "def train_lstm(p_n_symbols, p_embedding_weights, p_X_train, p_y_train, p_X_test, p_y_test):\n",
    "    print u'创建模型...'\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=p_n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[p_embedding_weights],\n",
    "                        input_length=input_length))\n",
    "\n",
    "    model.add(LSTM(output_dim=50,\n",
    "                   activation='sigmoid',\n",
    "                   inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print u'编译模型...'\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print u\"训练...\"\n",
    "    model.fit(p_X_train, p_y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
    "              validation_data=(p_X_test, p_y_test))\n",
    "\n",
    "    print u\"评估...\"\n",
    "    score, acc = model.evaluate(p_X_test, p_y_test, batch_size=batch_size)\n",
    "    print 'Test score:', score\n",
    "    print 'Test accuracy:', acc\n",
    "\n",
    "\n",
    "# 读取大语料文本\n",
    "f = open(u\"评价语料索引及词向量.pkl\", 'rb')  # 预先训练好的\n",
    "index_dict = pickle.load(f)  # 索引字典，{单词: 索引数字}\n",
    "word_vectors = pickle.load(f)  # 词向量, {单词: 词向量(100维长的数组)}\n",
    "new_dic = index_dict\n",
    "\n",
    "print u\"Setting up Arrays for Keras Embedding Layer...\"\n",
    "n_symbols = len(index_dict) + 1  # 索引数字的个数，因为有的词语索引为0，所以+1\n",
    "embedding_weights = np.zeros((n_symbols, 100))  # 创建一个n_symbols * 100的0矩阵\n",
    "for w, index in index_dict.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "    embedding_weights[index, :] = word_vectors[w]  # 词向量矩阵，第一行是0向量（没有索引为0的词语，未被填充）\n",
    "\n",
    "# 读取语料分词文本，转为句子列表（句子为词汇的列表）\n",
    "print u\"请选择语料的分词文本...\"\n",
    "T1 = TextSta(u\"评价语料_分词后.txt\")\n",
    "allsentences = T1.sen()\n",
    "\n",
    "# 读取语料类别标签\n",
    "print u\"请选择语料的类别文本...（用0，1分别表示消极、积极情感）\"\n",
    "labels = GetLineList.main()\n",
    "\n",
    "# 划分训练集和测试集，此时都是list列表\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(allsentences, labels, test_size=0.2)\n",
    "\n",
    "# 转为数字索引形式\n",
    "X_train = text_to_index_array(new_dic, X_train_l)\n",
    "X_test = text_to_index_array(new_dic, X_test_l)\n",
    "print u\"训练集shape： \", X_train.shape\n",
    "print u\"测试集shape： \", X_test.shape\n",
    "\n",
    "y_train = np.array(y_train_l)  # 转numpy数组\n",
    "y_test = np.array(y_test_l)\n",
    "\n",
    "# 将句子截取相同的长度maxlen，不够的补0\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "train_lstm(n_symbols, embedding_weights, X_train, y_train, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "42\n",
    "43\n",
    "44\n",
    "45\n",
    "46\n",
    "47\n",
    "48\n",
    "49\n",
    "50\n",
    "51\n",
    "52\n",
    "53\n",
    "54\n",
    "55\n",
    "56\n",
    "57\n",
    "58\n",
    "59\n",
    "60\n",
    "61\n",
    "62\n",
    "63\n",
    "64\n",
    "65\n",
    "66\n",
    "67\n",
    "68\n",
    "69\n",
    "70\n",
    "71\n",
    "72\n",
    "73\n",
    "74\n",
    "75\n",
    "76\n",
    "77\n",
    "78\n",
    "79\n",
    "80\n",
    "81\n",
    "82\n",
    "83\n",
    "84\n",
    "85\n",
    "86\n",
    "87\n",
    "88\n",
    "89\n",
    "90\n",
    "91\n",
    "92\n",
    "93\n",
    "94\n",
    "95\n",
    "96\n",
    "97\n",
    "98\n",
    "99\n",
    "100\n",
    "101\n",
    "102\n",
    "103\n",
    "104\n",
    "105\n",
    "106\n",
    "107\n",
    "108\n",
    "109\n",
    "110\n",
    "111\n",
    "112\n",
    "113\n",
    "114\n",
    "115\n",
    "116\n",
    "117\n",
    "118\n",
    "119\n",
    "120\n",
    "121\n",
    "122\n",
    "123\n",
    "其中，\n",
    "\n",
    "from Functions import GetLineList\n",
    "1\n",
    "GetLineList是自定义模块，用于获取文本的类别（存为列表），代码如下：\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "功能：文本转列表，常用于读取词典（停用词，特征词等）\n",
    "使用：给定一个文本，将文本按行转换为列表，每行对应列表里的一个元素\n",
    "时间：2016年5月15日 22:45:23\n",
    "\"\"\"\n",
    "\n",
    "import codecs\n",
    "import tkFileDialog\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 打开文件\n",
    "    file_path = tkFileDialog.askopenfilename(title=u\"选择文件\")\n",
    "    f1 = codecs.open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    print u\"已经打开文本：\", file_path\n",
    "\n",
    "    # 转为列表\n",
    "    line_list = []\n",
    "    for line in f1:\n",
    "        line_list.append(line.strip())\n",
    "    print u\"列表里的元素个数：\", len(line_list)\n",
    "\n",
    "    f1.close()\n",
    "    return line_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "实验结果\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
